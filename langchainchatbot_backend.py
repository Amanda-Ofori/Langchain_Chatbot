# -*- coding: utf-8 -*-
"""LangchainChatbot_Documentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dee8AnNpqMxUvnTQwvKtb-q-RRd0KeHy


"""

import os
import json
import fitz  # PyMuPDF
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import glob
import chardet  # Import chardet for encoding detection
import chardet
import sys




class PDFDocumentProcessor:
    def __init__(self, directory, storage_directory):
        self.directory = directory
        self.storage_directory = storage_directory
        self.text_splitter = RecursiveCharacterTextSplitter()
        self.ensure_directory_exists(self.storage_directory)

    def ensure_directory_exists(self, directory):
        """Ensure the storage directory exists."""
        if not os.path.exists(directory):
            os.makedirs(directory)

    def extract_text(self):
        """Extract text from PDF files in the directory and store it."""
        for filename in os.listdir(self.directory):
            if filename.endswith('.pdf'):
                path = os.path.join(self.directory, filename)
                with fitz.open(path) as doc:
                    text = " ".join(page.get_text() for page in doc)
                tokenized_text = self.text_splitter.split_text(text)
                self.store_document(tokenized_text, filename)

    def store_document(self, document, filename):
        """Store the tokenized document in a text file using UTF-8 encoding."""
        file_path = os.path.join(self.storage_directory, filename.replace('.pdf', '.txt'))
        with open(file_path, 'w', encoding='utf-8') as file:
            for item in document:
                file.write("%s\n" % item)



def read_file_safely(file_path):
    # Read the file as binary data for detection
    with open(file_path, 'rb') as file:
        raw_data = file.read()

    # Detect the encoding
    detected = chardet.detect(raw_data)
    encoding = detected['encoding'] if detected['encoding'] is not None else 'utf-8'

    try:
        # Attempt to read with detected encoding
        return raw_data.decode(encoding)
    except UnicodeDecodeError:
        # Fallback to UTF-8 and ignore errors
        return raw_data.decode('utf-8', errors='ignore')
             

class CustomRAGProcessor:
    def __init__(self, model_name, storage_directory):
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.model = GPT2LMHeadModel.from_pretrained(model_name)
        self.storage_directory = storage_directory

    def detect_encoding(self, file_path):
        with open(file_path, 'rb') as file:
            raw_data = file.read()
        result = chardet.detect(raw_data)
        return result['encoding']

    def retrieve_documents(self, query):
        documents = []
        for file_path in glob.glob(os.path.join(self.storage_directory, '*.txt')):
            document = read_file_safely(file_path)
            if query.lower() in document.lower():
                documents.append(document)
        return documents

    def generate_answer(self, query, max_length=100):
        documents = self.retrieve_documents(query)
        combined_context = " ".join(documents)
        # Truncate context to fit within model's maximum input size if necessary
        max_context_length = max_length - len(query) - 50  # reserve space for query and extra tokens
        if len(combined_context) > max_context_length:
            combined_context = combined_context[:max_context_length]

        input_text = f"Context: {combined_context} Question: {query}"
        input_ids = self.tokenizer.encode(input_text, return_tensors="pt", max_length=max_length, truncation=True)

        # Adjust generation parameters for more coherent generation
        output_ids = self.model.generate(
            input_ids,
            max_length=max_length,
            max_new_tokens=150,  # adjust as needed
            num_return_sequences=1,
            top_p=0.92,  # nucleus sampling, less randomness
            top_k=50,  # top-k sampling
            temperature=0.7  # lower temperature makes outputs less random
        )
        return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)

# Example usage
processor = PDFDocumentProcessor('Documents', 'path/to/storage')
processor.extract_text()

rag_processor = CustomRAGProcessor('gpt2', 'path/to/storage')
answer = rag_processor.generate_answer("What is the topic?")
print(answer)



#     rag_processor = CustomRAGProcessor("gpt2")

# pip install streamlit

# """## Streamlit Chatbot Interface for PDF Documents

# This code sets up a Streamlit web application for interacting with a PDF chatbot. It imports necessary modules and initializes instances of `PDFDocumentProcessor` and `CustomRAGProcessor`. The main function defines the layout of the Streamlit app, including a title, text input for user queries, and a button to trigger the chatbot response. When the button is clicked, the app extracts text from PDF documents using the `PDFDocumentProcessor` and generates a response using the `CustomRAGProcessor`. Finally, it displays the bot's response in a text area.
# """

# import streamlit as st
# # from langchainchatbot_backend import PDFDocumentProcessor, CustomRAGProcessor

# # Initialize the PDFDocumentProcessor and CustomRAGProcessor
# processor = PDFDocumentProcessor("Documents")
# rag_processor = CustomRAGProcessor("gpt2")

# # Define the Streamlit app
# def main():
#     st.title("PDF Chatbot")

#     # User input for query
#     user_query = st.text_input("Ask your question:")

#     # Button to trigger chatbot response
#     if st.button("Get Answer"):
#         # Extract text from PDF documents
#         documents = processor.extract_text()

#         # Generate response using the chatbot
#         response = rag_processor.generate_answer(user_query, documents)

#         # Display the response
#         st.text_area("Bot's Response:", value=response, height=200, max_chars=None)

# if __name__ == "__main__":
#     main()

# ! streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]